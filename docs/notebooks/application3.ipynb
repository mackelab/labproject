{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient ascent to visualize the effect of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "from labproject.embeddings import EmbeddingNet, FIDEmbeddingNet\n",
    "from labproject.external.inception_v3 import InceptionV3, get_inception_v3_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10('../../data', train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_class(dataset, class_idx, n_samples=16, to_tensor=True):\n",
    "    idxs = torch.where(torch.tensor(dataset.targets) == class_idx)[0]\n",
    "    idxs = idxs[torch.randperm(len(idxs))[:n_samples]]\n",
    "    if to_tensor:\n",
    "        return torch.stack([torchvision.transforms.ToTensor()(dataset[idx][0]) for idx in idxs])\n",
    "    return [dataset[idx][0] for idx in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoSampleSet(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset, class1, class2, n_samples=16, batch_size=4, two_requires_grad=True):\n",
    "        self.class1 = sample_from_class(dataset, class1, n_samples)\n",
    "        self.class2 = sample_from_class(dataset, class2, n_samples)\n",
    "        self.class2.requires_grad = two_requires_grad\n",
    "        self.n_samples = n_samples\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.class1[idx], self.class2[idx].clip(0, 1)\n",
    "\n",
    "    def plot_grid(self, which=1, width=4, height=4, figsize=[4, 4]):\n",
    "\n",
    "        if which == 1:\n",
    "            data = twosampleset.class1\n",
    "        elif which == 2:\n",
    "            data = twosampleset.class2\n",
    "\n",
    "        fig, ax = plt.subplots(width, height, figsize = figsize)\n",
    "\n",
    "        for idx, (i, j) in enumerate(product(range(width), range(height))):\n",
    "            try:\n",
    "                img = data[idx]\n",
    "            except IndexError:\n",
    "                break\n",
    "            if isinstance(img, torch.Tensor):\n",
    "                img = img.detach().numpy().transpose((1, 2, 0)).clip(0, 1)\n",
    "            ax[i, j].imshow(img)\n",
    "            ax[i, j].set_axis_off()\n",
    "        plt.subplots_adjust(wspace=0.1, hspace=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twosampleset = TwoSampleSet(dataset, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twosampleset.plot_grid(1, width=4, height=4)\n",
    "twosampleset.plot_grid(2, width=4, height=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(twosampleset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_net = InceptionV3()\n",
    "def get_inception_v3_activations_with_grad(model, images):\n",
    "    \"\"\"\n",
    "    Get activations from the InceptionV3 model for the given images.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The InceptionV3 model.\n",
    "        images (torch.Tensor): The images for which to compute the activations.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of torch.Tensor containing the activations from the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    pred = model(images)[0]\n",
    "    pred = pred.squeeze(3).squeeze(2).cpu()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam([dataloader.dataset.class2], lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.gaussian_squared_wasserstein.gaussian_squared_w2_distance(activations1, activations2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "losses = []\n",
    "\n",
    "for epochs in range(n_epochs):\n",
    "    for batch_class1, batch_class2 in dataloader:\n",
    "        optim.zero_grad()\n",
    "        activations1 = get_inception_v3_activations_with_grad(embedding_net, batch_class1)\n",
    "        activations2 = get_inception_v3_activations_with_grad(embedding_net, batch_class2)\n",
    "        loss = metrics.sliced_wasserstein.sliced_wasserstein_distance(activations1, activations2)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twosampleset.plot_grid(2, width=4, height=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
