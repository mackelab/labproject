{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"A Practical Guide to Statistical Distances for Evaluating Generative Models in Science","text":"<p>Generative models are highly useful in many disciplines of science. How do we evaluate them? As the data generated by these models is often high-dimensional and/or non-parametric, we can typically not resort to classical statistical tests. This paper aims to provide an accessible entry point to a understanding popular statistical distances proposed and adopted by the machine learning in science community, requiring only foundational knowledge in mathematics or statistics. We focus on four commonly used classes of statistical distances: obtaining a distance using classifiers (e.g. classifier two-sample tests), using embeddings through kernels (e.g. Maximum Mean Discrepancy) or neural networks (e.g. Frechet Inception Distance), and slicing (e.g. sliced Wasserstein). We highlight their merits, scalability, complexity and pitfalls, which are all illustrated in accompanying notebooks. We then use these metrics to compare generative models from two different scientific domains, primate decision modeling in cognitive neuroscience and chest x-rays in medical imaging. We this aim to empower researchers to use, critically assess and interpret statistical distances for generative models in science.</p>"},{"location":"#link-to-accompanying-paper","title":"Link to accompanying paper","text":"<p>click here to go to the arxiv document</p>"},{"location":"#installation","title":"Installation","text":"<p>Please execute the following commands in your terminal to install the labproject package and its dependencies. We recommend using a conda environment to avoid conflicts with other packages.</p> <pre><code># clone the repository\ngit clone https://github.com/mackelab/labproject.git\n\n# (optional but recommended) create conda environment\nconda create -n labproject python=3.9\nconda activate labproject\n\n# install labproject package with dependencies\npython3 -m pip install --upgrade pip\ncd labproject\npip install -e \".[dev,docs]\"\n\n# install pre-commit hooks for black auto-formatting\npre-commit install\n</code></pre> <p><code>pip install -e .</code> installs the labproject package in editable mode, i.e. changes to the code are immediately reflected in the package.</p> <p>The environment now contains, <code>numpy</code>, <code>scipy</code>, <code>matplotlib</code>, <code>torch</code>, and <code>jupyter</code>.</p>"},{"location":"#development","title":"Development","text":"<p>Develop code in your desired way, e.g. in local notebooks (you don't commit them, they will be automatically ignored). The public notebooks i.e. for the website can be found in <code>docs/notebooks/</code>, if you want to share the whole notebook then move it in this directory.</p>"},{"location":"#metrics","title":"Metrics","text":"<p>If you implemented a well-documented and reliable function that computes a metric, then move it to in <code>labproject/metrics</code> simply by add a new file <code>my_metric.py</code>.</p>"},{"location":"#plotting","title":"Plotting","text":"<p>If you implemented a nice plotting function, then move it to in <code>labproject/plotting.py</code>. Especially if it is applicable to multiple tasks.</p>"},{"location":"#running-experiments","title":"Running experiments","text":"<p>After committing and pushing your changes, GitHub will execute every <code>run_{name}.py</code> and update the figures in Overleaf. </p> <p>You can also run it yourself with <code>python labproject/run_{name}.py</code>. Any results, will however not be pushed into the repository.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>We use mkdocs to create the public version of our tutorials (notebooks) and the API documentation. mkdocs are written in Markdown and are found in <code>docs/</code>.</p> <p>After installing the necessary dependencies with <code>pip install -e \".[docs]\"</code>, you can view your local version with <code>mkdocs serve</code> in <code>labproject/</code> and then open http://127.0.0.1:8000/. Every push to the main branch will also publish the current version on: http://www.mackelab.org/labproject/ </p> <p>To add your notebooks as pages in the docs, add the following to <code>mkdocs.yml</code>: <pre><code>pages:\n  - Notebooks: \n    - Example Notebook: notebooks/example.ipynb\n    - FID notebook: notebooks/fid.ipynb\n    - UR_NOTEBOOK_NAME: notebook/UR_NOTEBOOK_FILE\n</code></pre></p> <p>To add your functions to the API documentation, add the following to <code>docs/api.md</code>: <pre><code>### Your module\n::: labproject.your_module\n    options:\n      heading_level: 4\n</code></pre> To build it locally use <code>mkdocs build</code> (or <code>mkdocs serve</code> to view it locally). After pushing to the main branch, the documentation will be automatically updated.</p> <p>For adding new pages, create a new markdown file in <code>docs/</code> and add it to <code>mkdocs.yml</code>: <pre><code>pages:\n  - 'your_new_page.md'\n</code></pre></p> <p>Every docstring in the code will be automatically added to the API documentation. We will use \"Google Style\" docstrings, see here for an example.</p>"},{"location":"#notebooks","title":"Notebooks","text":"<p>The jupyter notebooks are found in <code>docs/notebooks/</code>.</p> <p>For your convenience, at the beginning of the jupyter notebook, run   <pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> for automatic reloading of modules, in case you make any running changes to the labproject code.</p>"},{"location":"api/","title":"API reference for development package labproject","text":"<p>Here all functions will be documented that are part of the public API of the labproject package.</p>"},{"location":"api/#metrics","title":"Metrics","text":"<p>Best practices for developing metrics:</p> <ol> <li>Please do everything in torch, and if that is not possible, cast the output to torch.Tensor.</li> <li>The function should be well-documented, including type hints.</li> <li>The function should be tested with a simple example.</li> <li>Add an assert at the beginning for shape checking (N,D), see examples. </li> <li>Register the function by importing <code>labrpoject.metrics.utils.regiter_metric</code> and give your function a meaningful name.</li> </ol>"},{"location":"api/#gaussian-kl-divergence","title":"Gaussian KL divergence","text":""},{"location":"api/#labproject.metrics.gaussian_kl.gaussian_kl_divergence","title":"<code>gaussian_kl_divergence(real_samples, fake_samples)</code>","text":"<p>Compute the KL divergence between Gaussian approximations of real and fake samples. Dimensionality of the samples must be the same and &gt;=2 (for covariance calculation).</p> <p>In detail, for each set of samples, we calculate the mean and covariance matrix.</p> \\[ \\mu_{\\text{real}} = \\frac{1}{n} \\sum_{i=1}^{n} x_i \\qquad \\mu_{\\text{fake}} = \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] \\[ \\Sigma_{\\text{real}} = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\mu_{\\text{real}})(x_i - \\mu_{\\text{real}})^T \\qquad \\Sigma_{\\text{fake}} = \\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\mu_{\\text{fake}})(y_i - \\mu_{\\text{fake}})^T \\] <p>Then we calculate the KL divergence between the two Gaussian approximations:</p> \\[ D_{KL}(N(\\mu_{\\text{real}}, \\Sigma_{\\text{real}}) || N(\\mu_{\\text{fake}}, \\Sigma_{\\text{fake}})) = \\frac{1}{2} \\left( \\text{tr}(\\Sigma_{\\text{fake}}^{-1} \\Sigma_{\\text{real}}) + (\\mu_{\\text{fake}} - \\mu_{\\text{real}})^T \\Sigma_{\\text{fake}}^{-1} (\\mu_{\\text{fake}} - \\mu_{\\text{real}}) - k + \\log \\frac{|\\Sigma_{\\text{fake}}|}{|\\Sigma_{\\text{real}}|} \\right) \\] <p>Parameters:</p> Name Type Description Default <code>real_samples</code> <code>Tensor</code> <p>A tensor representing the real samples.</p> required <code>fake_samples</code> <code>Tensor</code> <p>A tensor representing the fake samples.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The KL divergence between the two Gaussian approximations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; real_samples = torch.randn(100, 2)  # 100 samples, 2-dimensional\n&gt;&gt;&gt; fake_samples = torch.randn(100, 2)  # 100 samples, 2-dimensional\n&gt;&gt;&gt; kl_div = gaussian_kl_divergence(real_samples, fake_samples)\n&gt;&gt;&gt; print(kl_div)\n</code></pre> Source code in <code>labproject/metrics/gaussian_kl.py</code> <pre><code>@register_metric(\"gaussian_kl_divergence\")\ndef gaussian_kl_divergence(real_samples: Tensor, fake_samples: Tensor) -&gt; Tensor:\n    r\"\"\"\n    Compute the KL divergence between Gaussian approximations of real and fake samples.\n    Dimensionality of the samples must be the same and &gt;=2 (for covariance calculation).\n\n    In detail, for each set of samples, we calculate the mean and covariance matrix.\n\n    $$ \\mu_{\\text{real}} = \\frac{1}{n} \\sum_{i=1}^{n} x_i \\qquad \\mu_{\\text{fake}} = \\frac{1}{n} \\sum_{i=1}^{n} y_i $$\n\n\n    $$\n    \\Sigma_{\\text{real}} = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\mu_{\\text{real}})(x_i - \\mu_{\\text{real}})^T \\qquad\n    \\Sigma_{\\text{fake}} = \\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\mu_{\\text{fake}})(y_i - \\mu_{\\text{fake}})^T\n    $$\n\n    Then we calculate the KL divergence between the two Gaussian approximations:\n\n    $$\n    D_{KL}(N(\\mu_{\\text{real}}, \\Sigma_{\\text{real}}) || N(\\mu_{\\text{fake}}, \\Sigma_{\\text{fake}})) =\n    \\frac{1}{2} \\left( \\text{tr}(\\Sigma_{\\text{fake}}^{-1} \\Sigma_{\\text{real}}) + (\\mu_{\\text{fake}} - \\mu_{\\text{real}})^T \\Sigma_{\\text{fake}}^{-1} (\\mu_{\\text{fake}} - \\mu_{\\text{real}})\n    - k + \\log \\frac{|\\Sigma_{\\text{fake}}|}{|\\Sigma_{\\text{real}}|} \\right)\n    $$\n\n    Args:\n        real_samples (torch.Tensor): A tensor representing the real samples.\n        fake_samples (torch.Tensor): A tensor representing the fake samples.\n\n    Returns:\n        torch.Tensor: The KL divergence between the two Gaussian approximations.\n\n    Examples:\n        &gt;&gt;&gt; real_samples = torch.randn(100, 2)  # 100 samples, 2-dimensional\n        &gt;&gt;&gt; fake_samples = torch.randn(100, 2)  # 100 samples, 2-dimensional\n        &gt;&gt;&gt; kl_div = gaussian_kl_divergence(real_samples, fake_samples)\n        &gt;&gt;&gt; print(kl_div)\n    \"\"\"\n\n    # check input (n,d only)\n    assert len(real_samples.size()) == 2, \"Real samples must be 2-dimensional, (n,d)\"\n    assert len(fake_samples.size()) == 2, \"Fake samples must be 2-dimensional, (n,d)\"\n\n    # calculate mean and covariance of real and fake samples\n    mu_real = real_samples.mean(dim=0)\n    mu_fake = fake_samples.mean(dim=0)\n    cov_real = torch.cov(real_samples.t())\n    cov_fake = torch.cov(fake_samples.t())\n\n    # ensure the covariance matrices are invertible\n    eps = 1e-8\n    cov_real += torch.eye(cov_real.size(0)) * eps\n    cov_fake += torch.eye(cov_fake.size(0)) * eps\n\n    # compute KL divergence\n    inv_cov_fake = torch.inverse(cov_fake)\n    kl_div = 0.5 * (\n        torch.trace(inv_cov_fake @ cov_real)\n        + (mu_fake - mu_real).dot(inv_cov_fake @ (mu_fake - mu_real))\n        - real_samples.size(1)\n        + torch.log(torch.det(cov_fake) / torch.det(cov_real))\n    )\n\n    return kl_div\n</code></pre>"},{"location":"api/#gaussian-wasserstein","title":"Gaussian Wasserstein","text":""},{"location":"api/#labproject.metrics.gaussian_squared_wasserstein.gaussian_squared_w2_distance","title":"<code>gaussian_squared_w2_distance(real_samples, fake_samples, real_mu=None, real_cov=None)</code>","text":"<p>Compute the squared Wasserstein distance between Gaussian approximations of real and fake samples. Dimensionality of the samples must be the same and &gt;=2 (for covariance calculation).</p> <p>In detail, for each set of samples, we calculate the mean and covariance matrix.</p> \\[ \\mu_{\\text{real}} = \\frac{1}{n} \\sum_{i=1}^{n} x_i \\qquad \\mu_{\\text{fake}} = \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] \\[ \\Sigma_{\\text{real}} = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\mu_{\\text{real}})(x_i - \\mu_{\\text{real}})^T \\qquad \\Sigma_{\\text{fake}} = \\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\mu_{\\text{fake}})(y_i - \\mu_{\\text{fake}})^T \\] <p>Then we calculate the squared Wasserstein distance between the two Gaussian approximations:</p> \\[ d_{W_2}^2(N(\\mu_{\\text{real}}, \\Sigma_{\\text{real}}), N(\\mu_{\\text{fake}}, \\Sigma_{\\text{fake}})) = \\left\\| \\mu_{\\text{real}} - \\mu_{\\text{fake}} \\right\\|^2 + \\text{tr}(\\Sigma_{\\text{real}} + \\Sigma_{\\text{fake}} - 2 \\sqrt{\\Sigma_{\\text{real}} \\Sigma_{\\text{fake}}}) \\] <p>Parameters:</p> Name Type Description Default <code>real_samples</code> <code>Tensor</code> <p>A tensor representing the real samples.</p> required <code>fake_samples</code> <code>Tensor</code> <p>A tensor representing the fake samples.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The KL divergence between the two Gaussian approximations.</p> References <p>[1] https://en.wikipedia.org/wiki/Wasserstein_metric [2] https://arxiv.org/pdf/1706.08500.pdf</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; real_samples = torch.randn(100, 2)  # 100 samples, 2-dimensional\n&gt;&gt;&gt; fake_samples = torch.randn(100, 2)  # 100 samples, 2-dimensional\n&gt;&gt;&gt; w2 = gaussian_squared_w2_distance(real_samples, fake_samples)\n&gt;&gt;&gt; print(w2)\n</code></pre> Source code in <code>labproject/metrics/gaussian_squared_wasserstein.py</code> <pre><code>@register_metric(\"wasserstein_gauss_squared\")\ndef gaussian_squared_w2_distance(\n    real_samples: Tensor, fake_samples: Tensor, real_mu=None, real_cov=None\n) -&gt; Tensor:\n    r\"\"\"\n    Compute the squared Wasserstein distance between Gaussian approximations of real and fake samples.\n    Dimensionality of the samples must be the same and &gt;=2 (for covariance calculation).\n\n    In detail, for each set of samples, we calculate the mean and covariance matrix.\n\n    $$ \\mu_{\\text{real}} = \\frac{1}{n} \\sum_{i=1}^{n} x_i \\qquad \\mu_{\\text{fake}} = \\frac{1}{n} \\sum_{i=1}^{n} y_i $$\n\n\n    $$\n    \\Sigma_{\\text{real}} = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\mu_{\\text{real}})(x_i - \\mu_{\\text{real}})^T \\qquad\n    \\Sigma_{\\text{fake}} = \\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\mu_{\\text{fake}})(y_i - \\mu_{\\text{fake}})^T\n    $$\n\n    Then we calculate the squared Wasserstein distance between the two Gaussian approximations:\n\n    $$\n    d_{W_2}^2(N(\\mu_{\\text{real}}, \\Sigma_{\\text{real}}), N(\\mu_{\\text{fake}}, \\Sigma_{\\text{fake}})) =\n    \\left\\| \\mu_{\\text{real}} - \\mu_{\\text{fake}} \\right\\|^2 + \\text{tr}(\\Sigma_{\\text{real}} + \\Sigma_{\\text{fake}} - 2 \\sqrt{\\Sigma_{\\text{real}} \\Sigma_{\\text{fake}}})\n    $$\n\n    Args:\n        real_samples (torch.Tensor): A tensor representing the real samples.\n        fake_samples (torch.Tensor): A tensor representing the fake samples.\n\n    Returns:\n        torch.Tensor: The KL divergence between the two Gaussian approximations.\n\n    References:\n        [1] https://en.wikipedia.org/wiki/Wasserstein_metric\n        [2] https://arxiv.org/pdf/1706.08500.pdf\n\n    Examples:\n        &gt;&gt;&gt; real_samples = torch.randn(100, 2)  # 100 samples, 2-dimensional\n        &gt;&gt;&gt; fake_samples = torch.randn(100, 2)  # 100 samples, 2-dimensional\n        &gt;&gt;&gt; w2 = gaussian_squared_w2_distance(real_samples, fake_samples)\n        &gt;&gt;&gt; print(w2)\n    \"\"\"\n\n    # check input (n,d only)\n    assert len(real_samples.size()) == 2, \"Real samples must be 2-dimensional, (n,d)\"\n    assert len(fake_samples.size()) == 2, \"Fake samples must be 2-dimensional, (n,d)\"\n\n    if real_samples.shape[-1] == 1:\n        mu_real = real_samples.mean(dim=0)\n        var_real = real_samples.var(dim=0)\n\n        mu_fake = fake_samples.mean(dim=0)\n        var_fake = fake_samples.var(dim=0)\n\n        w2_squared_dist = (mu_real - mu_fake) ** 2 + (\n            var_real + var_fake - 2 * (var_real * var_fake).sqrt()\n        )\n\n        return w2_squared_dist\n    else:\n        # calculate mean and covariance of real and fake samples\n        if real_mu is None:\n            mu_real = real_samples.mean(dim=0)\n        else:\n            mu_real = real_mu\n        if real_cov is None:\n            cov_real = torch.cov(real_samples.t())\n        else:\n            cov_real = real_cov\n\n        mu_fake = fake_samples.mean(dim=0)\n        cov_fake = torch.cov(fake_samples.t())\n\n        # ensure the covariance matrices are invertible\n        eps = 1e-6\n        cov_real += torch.eye(cov_real.size(0)) * eps\n        cov_fake += torch.eye(cov_fake.size(0)) * eps\n\n        # compute KL divergence\n        mean_dist = torch.norm(mu_real - mu_fake, p=2)\n        cov_sqrt = scipy.linalg.sqrtm((cov_real @ cov_fake).numpy())\n        # print(cov_sqrt.real)\n        cov_sqrt = torch.from_numpy(cov_sqrt.real)\n        cov_dist = torch.trace(cov_real + cov_fake - 2 * cov_sqrt)\n        w2_squared_dist = mean_dist**2 + cov_dist\n\n        return w2_squared_dist\n</code></pre>"},{"location":"api/#sliced-wasserstein","title":"Sliced Wasserstein","text":""},{"location":"api/#labproject.metrics.sliced_wasserstein.rand_projections","title":"<code>rand_projections(embedding_dim, num_samples)</code>","text":"<p>This function generates num_samples random samples from the latent space's unti sphere.r</p> <p>Parameters:</p> Name Type Description Default <code>embedding_dim</code> <code>int</code> <p>dimention of the embedding</p> required <code>sum_samples</code> <code>int</code> <p>number of samples</p> required Return <p>torch.tensor: tensor of size (num_samples, embedding_dim)</p> Source code in <code>labproject/metrics/sliced_wasserstein.py</code> <pre><code>def rand_projections(embedding_dim: int, num_samples: int):\n    \"\"\"\n    This function generates num_samples random samples from the latent space's unti sphere.r\n\n    Args:\n        embedding_dim (int): dimention of the embedding\n        sum_samples (int): number of samples\n\n    Return :\n        torch.tensor: tensor of size (num_samples, embedding_dim)\n    \"\"\"\n\n    ws = torch.randn((num_samples, embedding_dim))\n    projection = ws / torch.norm(ws, dim=-1, keepdim=True)\n    return projection\n</code></pre>"},{"location":"api/#labproject.metrics.sliced_wasserstein.sliced_wasserstein_distance","title":"<code>sliced_wasserstein_distance(encoded_samples, distribution_samples, num_projections=50, p=2, device='cpu')</code>","text":"<p>Sliced Wasserstein distance between encoded samples and distribution samples. Note that the SWD does not converge to the true Wasserstein distance, but rather it is a different proper distance metric.</p> <p>Parameters:</p> Name Type Description Default <code>encoded_samples</code> <code>Tensor</code> <p>tensor of encoded training samples</p> required <code>distribution_samples</code> <code>Tensor</code> <p>tensor drawn from the prior distribution</p> required <code>num_projection</code> <code>int</code> <p>number of projections to approximate sliced wasserstein distance</p> required <code>p</code> <code>int</code> <p>power of distance metric</p> <code>2</code> <code>device</code> <code>device</code> <p>torch device 'cpu' or 'cuda' gpu</p> <code>'cpu'</code> Return <p>torch.Tensor: Tensor of wasserstein distances of size (num_projections, 1)</p> Source code in <code>labproject/metrics/sliced_wasserstein.py</code> <pre><code>@register_metric(\"sliced_wasserstein\")\ndef sliced_wasserstein_distance(\n    encoded_samples: Tensor,\n    distribution_samples: Tensor,\n    num_projections: int = 50,\n    p: int = 2,\n    device: str = \"cpu\",\n) -&gt; Tensor:\n    \"\"\"\n    Sliced Wasserstein distance between encoded samples and distribution samples.\n    Note that the SWD does not converge to the true Wasserstein distance, but rather it is a different proper distance metric.\n\n    Args:\n        encoded_samples (torch.Tensor): tensor of encoded training samples\n        distribution_samples (torch.Tensor): tensor drawn from the prior distribution\n        num_projection (int): number of projections to approximate sliced wasserstein distance\n        p (int): power of distance metric\n        device (torch.device): torch device 'cpu' or 'cuda' gpu\n\n    Return:\n        torch.Tensor: Tensor of wasserstein distances of size (num_projections, 1)\n    \"\"\"\n\n    # check input (n,d only)\n    assert len(encoded_samples.size()) == 2, \"Real samples must be 2-dimensional, (n,d)\"\n    assert len(distribution_samples.size()) == 2, \"Fake samples must be 2-dimensional, (n,d)\"\n\n    embedding_dim = distribution_samples.size(-1)\n\n    projections = rand_projections(embedding_dim, num_projections).to(device)\n\n    encoded_projections = encoded_samples.matmul(projections.transpose(-2, -1))\n\n    distribution_projections = distribution_samples.matmul(projections.transpose(-2, -1))\n\n    wasserstein_distance = (\n        torch.sort(encoded_projections.transpose(-2, -1), dim=-1)[0]\n        - torch.sort(distribution_projections.transpose(-2, -1), dim=-1)[0]\n    )\n\n    wasserstein_distance = torch.pow(torch.abs(wasserstein_distance), p)\n\n    return torch.pow(torch.mean(wasserstein_distance, dim=(-2, -1)), 1 / p)\n</code></pre>"},{"location":"api/#main-modules","title":"Main Modules","text":""},{"location":"api/#data","title":"Data","text":""},{"location":"api/#labproject.data.download_file","title":"<code>download_file(remote_path, local_path)</code>","text":"<p>Downloads a file from the Hetzner Storage Box.</p> <p>Parameters:</p> Name Type Description Default <code>remote_path</code> <code>str</code> <p>The path to the remote file to be downloaded.</p> required <code>local_path</code> <code>str</code> <p>The path where the file should be saved locally.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the download is successful, False otherwise.</p> Example <p>if download_file('path/to/remote/file.txt', 'path/to/save/file.txt'):     print(\"Download successful\") else:     print(\"Download failed\")</p> Source code in <code>labproject/data.py</code> <pre><code>def download_file(remote_path, local_path):\n    r\"\"\"\n    Downloads a file from the Hetzner Storage Box.\n\n    Args:\n        remote_path (str): The path to the remote file to be downloaded.\n        local_path (str): The path where the file should be saved locally.\n\n    Returns:\n        bool: True if the download is successful, False otherwise.\n\n    Example:\n        &gt;&gt;&gt; if download_file('path/to/remote/file.txt', 'path/to/save/file.txt'):\n        &gt;&gt;&gt;     print(\"Download successful\")\n        &gt;&gt;&gt; else:\n        &gt;&gt;&gt;     print(\"Download failed\")\n    \"\"\"\n    url = f\"{STORAGEBOX_URL}/remote.php/dav/files/{HETZNER_STORAGEBOX_USERNAME}/{remote_path}\"\n    auth = HTTPBasicAuth(HETZNER_STORAGEBOX_USERNAME, HETZNER_STORAGEBOX_PASSWORD)\n    response = requests.get(url, auth=auth)\n    if response.status_code == 200:\n        with open(local_path, \"wb\") as f:\n            f.write(response.content)\n        return True\n    return False\n</code></pre>"},{"location":"api/#labproject.data.get_dataset","title":"<code>get_dataset(name)</code>","text":"<p>Get a dataset by name</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the dataset</p> required <code>n</code> <code>int</code> <p>Number of samples</p> required <code>d</code> <code>int</code> <p>Dimensionality of the samples</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Dataset</p> Source code in <code>labproject/data.py</code> <pre><code>def get_dataset(name: str) -&gt; torch.Tensor:\n    r\"\"\"Get a dataset by name\n\n    Args:\n        name (str): Name of the dataset\n        n (int): Number of samples\n        d (int): Dimensionality of the samples\n\n    Returns:\n        torch.Tensor: Dataset\n    \"\"\"\n    assert name in DATASETS, f\"Dataset {name} not found, please register it first \"\n    return DATASETS[name]\n</code></pre>"},{"location":"api/#labproject.data.get_distribution","title":"<code>get_distribution(name)</code>","text":"<p>Get a distribution by name</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the distribution</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Distribution</p> Source code in <code>labproject/data.py</code> <pre><code>def get_distribution(name: str) -&gt; torch.Tensor:\n    r\"\"\"Get a distribution by name\n\n    Args:\n        name (str): Name of the distribution\n\n    Returns:\n        torch.Tensor: Distribution\n    \"\"\"\n    assert name in DISTRIBUTIONS, f\"Distribution {name} not found, please register it first \"\n    return DISTRIBUTIONS[name]\n</code></pre>"},{"location":"api/#labproject.data.imagenet_conditional_model","title":"<code>imagenet_conditional_model(n, d=2048, label=None, device='cpu', permute_if_no_label=True, save_path='data')</code>","text":"<p>Get the conditional model embeddings for ImageNet</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of samples</p> required <code>d</code> <code>int</code> <p>Dimensionality of the embeddings. Defaults to 2048.</p> <code>2048</code> <code>label</code> <code>int</code> <p>Label, if None it takes random samples. Defaults to None.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device. Defaults to \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Type Description <p>torch.Tensor: ImageNet embeddings</p> Source code in <code>labproject/data.py</code> <pre><code>@register_dataset(\"imagenet_conditional_model\")\ndef imagenet_conditional_model(\n    n, d=2048, label: Optional[int] = None, device=\"cpu\", permute_if_no_label=True, save_path=\"data\"\n):\n    r\"\"\"Get the conditional model embeddings for ImageNet\n\n    Args:\n        n (int): Number of samples\n        d (int, optional): Dimensionality of the embeddings. Defaults to 2048.\n        label (int, optional): Label, if None it takes random samples. Defaults to None.\n        device (str, optional): Device. Defaults to \"cpu\".\n\n    Returns:\n        torch.Tensor: ImageNet embeddings\n    \"\"\"\n    assert d == 2048, \"The dimensionality of the embeddings must be 2048\"\n    if not os.path.exists(\"imagenet_conditional_model.pt\"):\n        import gdown\n\n        gdown.download(IMAGENET_CONDITIONAL_MODEL, \"imagenet_conditional_model.pt\", quiet=False)\n    conditional_embeddings = torch.load(\"imagenet_conditional_model.pt\")\n\n    if label is not None:\n        conditional_embeddings = conditional_embeddings[label]\n    else:\n        conditional_embeddings = conditional_embeddings.flatten(0, 1)\n        if permute_if_no_label:\n            conditional_embeddings = conditional_embeddings[\n                torch.randperm(conditional_embeddings.shape[0])\n            ]\n\n    max_n = conditional_embeddings.shape[0]\n\n    assert n &lt;= max_n, f\"Requested {n} samples, but only {max_n} are available\"\n\n    return conditional_embeddings[:n]\n</code></pre>"},{"location":"api/#labproject.data.imagenet_test_embedding","title":"<code>imagenet_test_embedding(n, d=2048, device='cpu', save_path='data')</code>","text":"<p>Get the test embeddings for ImageNet</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of samples</p> required <code>d</code> <code>int</code> <p>Dimensionality of the embeddings. Defaults to 2048.</p> <code>2048</code> <code>device</code> <code>str</code> <p>Device. Defaults to \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Type Description <p>torch.Tensor: ImageNet embeddings</p> Source code in <code>labproject/data.py</code> <pre><code>@register_dataset(\"imagenet_test_embedding\")\ndef imagenet_test_embedding(n, d=2048, device=\"cpu\", save_path=\"data\"):\n    r\"\"\"Get the test embeddings for ImageNet\n\n    Args:\n        n (int): Number of samples\n        d (int, optional): Dimensionality of the embeddings. Defaults to 2048.\n        device (str, optional): Device. Defaults to \"cpu\".\n\n    Returns:\n        torch.Tensor: ImageNet embeddings\n    \"\"\"\n    assert d == 2048, \"The dimensionality of the embeddings must be 2048\"\n    if not os.path.exists(\"imagenet_test_embedding.pt\"):\n        import gdown\n\n        gdown.download(IMAGENET_TEST_EMBEDDING, \"imagenet_test_embedding.pt\", quiet=False)\n    test_embeddigns = torch.load(\"imagenet_test_embedding.pt\")\n\n    max_n = test_embeddigns.shape[0]\n\n    assert n &lt;= max_n, f\"Requested {n} samples, but only {max_n} are available\"\n\n    return test_embeddigns[:n]\n</code></pre>"},{"location":"api/#labproject.data.imagenet_unconditional_model_embedding","title":"<code>imagenet_unconditional_model_embedding(n, d=2048, device='cpu', save_path='data')</code>","text":"<p>Get the unconditional model embeddings for ImageNet</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of samples</p> required <code>d</code> <code>int</code> <p>Dimensionality of the embeddings. Defaults to 2048.</p> <code>2048</code> <code>device</code> <code>str</code> <p>Device. Defaults to \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Type Description <p>torch.Tensor: ImageNet embeddings</p> Source code in <code>labproject/data.py</code> <pre><code>@register_dataset(\"imagenet_unconditional_model_embedding\")\ndef imagenet_unconditional_model_embedding(n, d=2048, device=\"cpu\", save_path=\"data\"):\n    r\"\"\"Get the unconditional model embeddings for ImageNet\n\n    Args:\n        n (int): Number of samples\n        d (int, optional): Dimensionality of the embeddings. Defaults to 2048.\n        device (str, optional): Device. Defaults to \"cpu\".\n\n    Returns:\n        torch.Tensor: ImageNet embeddings\n    \"\"\"\n    assert d == 2048, \"The dimensionality of the embeddings must be 2048\"\n    if not os.path.exists(\"imagenet_unconditional_model_embedding.pt\"):\n        import gdown\n\n        gdown.download(\n            IMAGENET_UNCONDITIONAL_MODEL_EMBEDDING,\n            \"imagenet_unconditional_model_embedding.pt\",\n            quiet=False,\n        )\n    unconditional_embeddigns = torch.load(\"imagenet_unconditional_model_embedding.pt\")\n\n    max_n = unconditional_embeddigns.shape[0]\n\n    assert n &lt;= max_n, f\"Requested {n} samples, but only {max_n} are available\"\n\n    return unconditional_embeddigns[:n]\n</code></pre>"},{"location":"api/#labproject.data.imagenet_validation_embedding","title":"<code>imagenet_validation_embedding(n, d=2048, device='cpu', save_path='data')</code>","text":"<p>Get the validation embeddings for ImageNet</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of samples</p> required <code>d</code> <code>int</code> <p>Dimensionality of the embeddings. Defaults to 2048.</p> <code>2048</code> <code>device</code> <code>str</code> <p>Device. Defaults to \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Type Description <p>torch.Tensor: ImageNet embeddings</p> Source code in <code>labproject/data.py</code> <pre><code>@register_dataset(\"imagenet_validation_embedding\")\ndef imagenet_validation_embedding(n, d=2048, device=\"cpu\", save_path=\"data\"):\n    r\"\"\"Get the validation embeddings for ImageNet\n\n    Args:\n        n (int): Number of samples\n        d (int, optional): Dimensionality of the embeddings. Defaults to 2048.\n        device (str, optional): Device. Defaults to \"cpu\".\n\n    Returns:\n        torch.Tensor: ImageNet embeddings\n    \"\"\"\n    assert d == 2048, \"The dimensionality of the embeddings must be 2048\"\n    if not os.path.exists(\"imagenet_validation_embedding.pt\"):\n        import gdown\n\n        gdown.download(\n            IMAGENET_VALIDATION_EMBEDDING, \"imagenet_validation_embedding.pt\", quiet=False\n        )\n    validation_embeddigns = torch.load(\"imagenet_validation_embedding.pt\")\n\n    max_n = validation_embeddigns.shape[0]\n\n    assert n &lt;= max_n, f\"Requested {n} samples, but only {max_n} are available\"\n\n    return validation_embeddigns[:n]\n</code></pre>"},{"location":"api/#labproject.data.load_cifar10","title":"<code>load_cifar10(n, save_path='data', train=True, batch_size=100, shuffle=False, num_workers=1, device='cpu', return_labels=False)</code>","text":"<p>Load a subset of cifar10</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of samples to load</p> required <code>save_path</code> <code>str</code> <p>Path to save files. Defaults to \"data\".</p> <code>'data'</code> <code>train</code> <code>bool</code> <p>Train or test. Defaults to True.</p> <code>True</code> <code>batch_size</code> <code>int</code> <p>Batch size. Defaults to 100.</p> <code>100</code> <code>shuffle</code> <code>bool</code> <p>Shuffle. Defaults to False.</p> <code>False</code> <code>num_workers</code> <code>int</code> <p>Parallel workers. Defaults to 1.</p> <code>1</code> <code>device</code> <code>str</code> <p>Device. Defaults to \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Cifar10 embeddings</p> Source code in <code>labproject/data.py</code> <pre><code>def load_cifar10(\n    n: int,\n    save_path=\"data\",\n    train=True,\n    batch_size=100,\n    shuffle=False,\n    num_workers=1,\n    device=\"cpu\",\n    return_labels=False,\n) -&gt; torch.Tensor:\n    \"\"\"Load a subset of cifar10\n\n    Args:\n        n (int): Number of samples to load\n        save_path (str, optional): Path to save files. Defaults to \"data\".\n        train (bool, optional): Train or test. Defaults to True.\n        batch_size (int, optional): Batch size. Defaults to 100.\n        shuffle (bool, optional): Shuffle. Defaults to False.\n        num_workers (int, optional): Parallel workers. Defaults to 1.\n        device (str, optional): Device. Defaults to \"cpu\".\n\n    Returns:\n        torch.Tensor: Cifar10 embeddings\n    \"\"\"\n    transform = transforms.Compose(\n        [\n            transforms.ToTensor(),\n        ]\n    )\n    cifar10 = CIFAR10(root=save_path, train=train, download=True, transform=transform)\n    dataloader = torch.utils.data.DataLoader(\n        cifar10, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers\n    )\n    dataset_subset = Subset(dataloader.dataset, range(n))\n    dataloader = DataLoader(\n        dataset_subset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers\n    )\n    net = FIDEmbeddingNet(device=device)\n    if return_labels:\n        embeddings, labels = net.get_embeddings_with_labels(dataloader)\n        return embeddings, labels\n    embeddings = net.get_embeddings(dataloader)\n    return embeddings\n</code></pre>"},{"location":"api/#labproject.data.register_dataset","title":"<code>register_dataset(name)</code>","text":"<p>This decorator wrapps a function that should return a dataset and ensures that the dataset is a PyTorch tensor, with the correct shape.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>Dataset generator function</p> required <p>Returns:</p> Name Type Description <code>callable</code> <code>callable</code> <p>Dataset generator function wrapper</p> Example <p>@register_dataset(\"random\") def random_dataset(n=1000, d=10):     return torch.randn(n, d)</p> Source code in <code>labproject/data.py</code> <pre><code>def register_dataset(name: str) -&gt; callable:\n    r\"\"\"This decorator wrapps a function that should return a dataset and ensures that the dataset is a PyTorch tensor, with the correct shape.\n\n    Args:\n        func (callable): Dataset generator function\n\n    Returns:\n        callable: Dataset generator function wrapper\n\n    Example:\n        &gt;&gt;&gt; @register_dataset(\"random\")\n        &gt;&gt;&gt; def random_dataset(n=1000, d=10):\n        &gt;&gt;&gt;     return torch.randn(n, d)\n    \"\"\"\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(n: int, d: Optional[int] = None, **kwargs):\n\n            assert n &gt; 0, \"n must be a positive integer\"\n            if d is not None:\n                assert d &gt; 0, \"d must be a positive integer\"\n            else:\n                warnings.warn(\"d is not specified, make sure you know what you're doing!\")\n\n            # Call the original function\n            if d is not None:\n                dataset = func(n, d, **kwargs)\n            else:\n                dataset = func(n, **kwargs)\n            if isinstance(dataset, tuple):\n                dataset = tuple(\n                    torch.Tensor(data) if not isinstance(data, torch.Tensor) else data\n                    for data in dataset\n                )\n            else:\n                dataset = (\n                    torch.Tensor(dataset) if not isinstance(dataset, torch.Tensor) else dataset\n                )\n            if d is not None:\n                assert dataset.shape == (n, d), f\"Dataset shape must be {(n, d)}\"\n            else:\n                assert dataset.shape[0] == n, f\"Dataset shape must be {(n, ...)}\"\n\n            return dataset\n\n        DATASETS[name] = wrapper\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/#labproject.data.register_distribution","title":"<code>register_distribution(name)</code>","text":"<p>This decorator wrapps a function that should return a dataset and ensures that the dataset is a PyTorch tensor, with the correct shape.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>Dataset generator function</p> required <p>Returns:</p> Name Type Description <code>callable</code> <code>callable</code> <p>Dataset generator function wrapper</p> Example <p>@register_dataset(\"random\") def random_dataset(n=1000, d=10):     return torch.randn(n, d)</p> Source code in <code>labproject/data.py</code> <pre><code>def register_distribution(name: str) -&gt; callable:\n    r\"\"\"This decorator wrapps a function that should return a dataset and ensures that the dataset is a PyTorch tensor, with the correct shape.\n\n    Args:\n        func (callable): Dataset generator function\n\n    Returns:\n        callable: Dataset generator function wrapper\n\n    Example:\n        &gt;&gt;&gt; @register_dataset(\"random\")\n        &gt;&gt;&gt; def random_dataset(n=1000, d=10):\n        &gt;&gt;&gt;     return torch.randn(n, d)\n    \"\"\"\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Call the original function\n            distribution = func(*args, **kwargs)\n            return distribution\n\n        DISTRIBUTIONS[name] = wrapper\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/#labproject.data.upload_file","title":"<code>upload_file(local_path, remote_path)</code>","text":"<p>Uploads a file to the Hetzner Storage Box.</p> <p>Parameters:</p> Name Type Description Default <code>local_path</code> <code>str</code> <p>The path to the local file to be uploaded.</p> required <code>remote_path</code> <code>str</code> <p>The path where the file should be uploaded on the remote server.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the upload is successful, False otherwise.</p> Example <p>if upload_file('path/to/your/local/file.txt', 'path/to/remote/file.txt'):     print(\"Upload successful\") else:     print(\"Upload failed\")</p> Source code in <code>labproject/data.py</code> <pre><code>def upload_file(local_path: str, remote_path: str):\n    r\"\"\"\n    Uploads a file to the Hetzner Storage Box.\n\n    Args:\n        local_path (str): The path to the local file to be uploaded.\n        remote_path (str): The path where the file should be uploaded on the remote server.\n\n    Returns:\n        bool: True if the upload is successful, False otherwise.\n\n    Example:\n        &gt;&gt;&gt; if upload_file('path/to/your/local/file.txt', 'path/to/remote/file.txt'):\n        &gt;&gt;&gt;     print(\"Upload successful\")\n        &gt;&gt;&gt; else:\n        &gt;&gt;&gt;     print(\"Upload failed\")\n    \"\"\"\n    url = f\"{STORAGEBOX_URL}/remote.php/dav/files/{HETZNER_STORAGEBOX_USERNAME}/{remote_path}\"\n    auth = HTTPBasicAuth(HETZNER_STORAGEBOX_USERNAME, HETZNER_STORAGEBOX_PASSWORD)\n    with open(local_path, \"rb\") as f:\n        data = f.read()\n    response = requests.put(url, data=data, auth=auth)\n    return response.status_code == 201\n</code></pre>"},{"location":"api/#embeddings","title":"Embeddings","text":"<p>This contains embedding nets, and auxilliary functions for extracting (N,D) embeddings from respective data and models.</p>"},{"location":"api/#experiments","title":"Experiments","text":""},{"location":"api/#labproject.experiments.ScaleDim","title":"<code>ScaleDim</code>","text":"<p>             Bases: <code>Experiment</code></p> Source code in <code>labproject/experiments.py</code> <pre><code>class ScaleDim(Experiment):\n    def __init__(self, metric_name, metric_fn, dim_sizes=None, min_dim=1, max_dim=1000, step=100):\n        self.metric_name = metric_name\n        self.metric_fn = metric_fn\n        if dim_sizes is not None:\n            self.dim_sizes = dim_sizes\n        else:\n            self.dim_sizes = list(range(min_dim, max_dim, step))\n        super().__init__()\n\n    def run_experiment(self, dataset1, dataset2, dataset_size, nb_runs=5, dim_sizes=None, **kwargs):\n        final_distances = []\n        final_errors = []\n        n = dataset_size\n        if dim_sizes is None:\n            dim_sizes = self.dim_sizes\n        for idx in range(nb_runs):\n            distances = []\n            for d in dim_sizes:\n                # 3000 x 100\n                data1 = dataset1[torch.randperm(dataset1.size(0))[:n], :d]\n                data2 = dataset2[\n                    torch.randperm(dataset2.size(0))[:n], :d\n                ]  # AS: changed from dataset1 to dataset2 in randperm\n                distances.append(self.metric_fn(data1, data2, **kwargs))\n            final_distances.append(distances)\n        final_distances = torch.transpose(torch.tensor(final_distances), 0, 1)\n        final_errors = (\n            torch.tensor([torch.std(d) for d in final_distances])\n            if nb_runs &gt; 1\n            else torch.zeros_like(torch.tensor(dim_sizes))\n        )\n        final_distances = torch.tensor([torch.mean(d) for d in final_distances])\n        return dim_sizes, final_distances, final_errors\n\n    def plot_experiment(\n        self,\n        dim_sizes,\n        distances,\n        errors,\n        dataset_name,\n        ax=None,\n        color=None,\n        label=None,\n        linestyle=\"-\",\n        **kwargs,\n    ):\n\n        plot_scaling_metric_dimensionality(\n            dim_sizes,\n            distances,\n            errors,\n            self.metric_name,\n            dataset_name,\n            ax=ax,\n            color=color,\n            label=label,\n            linestyle=linestyle,\n            **kwargs,\n        )\n\n    def log_results(self, results, log_path):\n        \"\"\"\n        Save the results to a file.\n        \"\"\"\n        with open(log_path, \"wb\") as f:\n            pickle.dump(results, f)\n</code></pre>"},{"location":"api/#labproject.experiments.ScaleDim.log_results","title":"<code>log_results(results, log_path)</code>","text":"<p>Save the results to a file.</p> Source code in <code>labproject/experiments.py</code> <pre><code>def log_results(self, results, log_path):\n    \"\"\"\n    Save the results to a file.\n    \"\"\"\n    with open(log_path, \"wb\") as f:\n        pickle.dump(results, f)\n</code></pre>"},{"location":"api/#labproject.experiments.ScaleHyperparameter","title":"<code>ScaleHyperparameter</code>","text":"<p>             Bases: <code>Experiment</code></p> Source code in <code>labproject/experiments.py</code> <pre><code>class ScaleHyperparameter(Experiment):\n    def __init__(\n        self, metric_name, metric_fn, value_sizes=None, min_value=0.2, max_value=50, step=10\n    ):\n        self.metric_name = metric_name\n        self.metric_fn = metric_fn\n        if value_sizes is not None:\n            self.value_sizes = value_sizes\n        else:\n            self.value_sizes = list(np.linspace(min_value, max_value, step))\n        super().__init__()\n\n    def run_experiment(self, dataset1, dataset2, nb_runs=5, n=10000, value_sizes=None, **kwargs):\n        final_distances = []\n        final_errors = []\n        # n = 1000 # AS: turned into argument\n        if value_sizes is None:\n            value_sizes = self.value_sizes\n            # print(value_sizes)\n        for idx in range(nb_runs):\n            distances = []\n            for v in value_sizes:\n                # print(v)\n                # 3000 x 100\n                data1 = dataset1[torch.randperm(dataset1.size(0))[:n], :]\n                data2 = dataset2[\n                    torch.randperm(dataset2.size(0))[:n], :\n                ]  # AS: changed from dataset1 to dataset2 in randperm\n                distances.append(self.metric_fn(data1, data2, v, **kwargs))\n\n            final_distances.append(distances)\n\n        final_distances = torch.transpose(torch.tensor(final_distances), 0, 1)\n        final_errors = (\n            torch.tensor([torch.std(d) for d in final_distances])\n            if nb_runs &gt; 1\n            else torch.zeros_like(torch.tensor(value_sizes))\n        )\n        final_distances = torch.tensor([torch.mean(d) for d in final_distances])\n        return value_sizes, final_distances, final_errors\n\n    def plot_experiment(\n        self,\n        value_sizes,\n        distances,\n        errors,\n        dataset_name,\n        ax=None,\n        color=None,\n        label=None,\n        linestyle=\"-\",\n        **kwargs,\n    ):\n\n        plot_scaling_metric_dimensionality(\n            value_sizes,\n            distances,\n            errors,\n            self.metric_name,\n            dataset_name,\n            ax=ax,\n            color=color,\n            label=label,\n            linestyle=linestyle,\n            **kwargs,\n        )\n\n    def log_results(self, results, log_path):\n        \"\"\"\n        Save the results to a file.\n        \"\"\"\n        with open(log_path, \"wb\") as f:\n            pickle.dump(results, f)\n</code></pre>"},{"location":"api/#labproject.experiments.ScaleHyperparameter.log_results","title":"<code>log_results(results, log_path)</code>","text":"<p>Save the results to a file.</p> Source code in <code>labproject/experiments.py</code> <pre><code>def log_results(self, results, log_path):\n    \"\"\"\n    Save the results to a file.\n    \"\"\"\n    with open(log_path, \"wb\") as f:\n        pickle.dump(results, f)\n</code></pre>"},{"location":"api/#labproject.experiments.ScaleSampleSize","title":"<code>ScaleSampleSize</code>","text":"<p>             Bases: <code>Experiment</code></p> Source code in <code>labproject/experiments.py</code> <pre><code>class ScaleSampleSize(Experiment):\n\n    def __init__(\n        self, metric_name, metric_fn, min_samples=3, max_samples=2000, step=100, sample_sizes=None\n    ):\n        assert min_samples &gt; 2, \"min_samples must be greater than 2 to compute covariance for KL\"\n        self.metric_name = metric_name\n        self.metric_fn = metric_fn\n        # TODO: add logarithmic scale or only keep pass in run experiment\n        if sample_sizes is not None:\n            self.sample_sizes = sample_sizes\n        else:\n            self.sample_sizes = list(range(min_samples, max_samples, step))\n        super().__init__()\n\n    def run_experiment(self, dataset1, dataset2, nb_runs=5, sample_sizes=None, **kwargs):\n        \"\"\"\n        Computes for each subset 5 different random subsets and averages performance across the subsets.\n        \"\"\"\n        final_distances = []\n        final_errors = []\n        if sample_sizes is None:\n            sample_sizes = self.sample_sizes\n        for idx in range(nb_runs):\n            distances = []\n            for n in sample_sizes:\n                data1 = dataset1[torch.randperm(dataset1.size(0))[:n], :]\n                data2 = dataset2[torch.randperm(dataset2.size(0))[:n], :]\n                distances.append(self.metric_fn(data1, data2, **kwargs))\n            final_distances.append(distances)\n        final_distances = torch.transpose(torch.tensor(final_distances), 0, 1)\n        final_errors = (\n            torch.tensor([torch.std(d) for d in final_distances])\n            if nb_runs &gt; 1\n            else torch.zeros_like(torch.tensor(sample_sizes))\n        )\n        final_distances = torch.tensor([torch.mean(d) for d in final_distances])\n\n        return sample_sizes, final_distances, final_errors\n\n    def plot_experiment(\n        self,\n        sample_sizes,\n        distances,\n        errors,\n        dataset_name,\n        ax=None,\n        color=None,\n        label=None,\n        linestyle=\"-\",\n        **kwargs,\n    ):\n        plot_scaling_metric_sample_size(\n            sample_sizes,\n            distances,\n            errors,\n            self.metric_name,\n            dataset_name,\n            ax=ax,\n            color=color,\n            label=label,\n            linestyle=linestyle,\n            **kwargs,\n        )\n\n    def log_results(self, results, log_path):\n        \"\"\"\n        Save the results to a file.\n        \"\"\"\n        with open(log_path, \"wb\") as f:\n            pickle.dump(results, f)\n</code></pre>"},{"location":"api/#labproject.experiments.ScaleSampleSize.log_results","title":"<code>log_results(results, log_path)</code>","text":"<p>Save the results to a file.</p> Source code in <code>labproject/experiments.py</code> <pre><code>def log_results(self, results, log_path):\n    \"\"\"\n    Save the results to a file.\n    \"\"\"\n    with open(log_path, \"wb\") as f:\n        pickle.dump(results, f)\n</code></pre>"},{"location":"api/#labproject.experiments.ScaleSampleSize.run_experiment","title":"<code>run_experiment(dataset1, dataset2, nb_runs=5, sample_sizes=None, **kwargs)</code>","text":"<p>Computes for each subset 5 different random subsets and averages performance across the subsets.</p> Source code in <code>labproject/experiments.py</code> <pre><code>def run_experiment(self, dataset1, dataset2, nb_runs=5, sample_sizes=None, **kwargs):\n    \"\"\"\n    Computes for each subset 5 different random subsets and averages performance across the subsets.\n    \"\"\"\n    final_distances = []\n    final_errors = []\n    if sample_sizes is None:\n        sample_sizes = self.sample_sizes\n    for idx in range(nb_runs):\n        distances = []\n        for n in sample_sizes:\n            data1 = dataset1[torch.randperm(dataset1.size(0))[:n], :]\n            data2 = dataset2[torch.randperm(dataset2.size(0))[:n], :]\n            distances.append(self.metric_fn(data1, data2, **kwargs))\n        final_distances.append(distances)\n    final_distances = torch.transpose(torch.tensor(final_distances), 0, 1)\n    final_errors = (\n        torch.tensor([torch.std(d) for d in final_distances])\n        if nb_runs &gt; 1\n        else torch.zeros_like(torch.tensor(sample_sizes))\n    )\n    final_distances = torch.tensor([torch.mean(d) for d in final_distances])\n\n    return sample_sizes, final_distances, final_errors\n</code></pre>"},{"location":"api/#plotting","title":"Plotting","text":""},{"location":"api/#labproject.plotting.place_boxplot","title":"<code>place_boxplot(ax, x, y, body_face_color='#8189c9', body_edge_color='k', body_lw=0.25, body_alpha=1.0, body_zorder=0, whisker_color='k', whisker_alpha=1.0, whisker_lw=1, whisker_zorder=1, cap_color='k', cap_lw=0.25, cap_zorder=1, median_color='k', median_alpha=1.0, median_lw=1.5, median_bar_length=1.0, median_zorder=10, width=0.5, scatter_face_color='k', scatter_edge_color='none', scatter_radius=5, scatter_lw=0.25, scatter_alpha=0.35, scatter_width=0.5, scatter=True, scatter_zorder=3, fill_box=True, showcaps=False, showfliers=False, whis=(0, 100), vert=True)</code>","text":"Example <p>X = [1, 2] Y = [np.random.normal(0.75, 0.12, size=50), np.random.normal(0.8, 0.20, size=25)] fig, ax = plt.subplots(figsize=[1, 1]) for (x, y) in zip(X, Y):     place_boxplot(ax, x, y)</p> Source code in <code>labproject/plotting.py</code> <pre><code>def place_boxplot(\n    ax,\n    x,\n    y,\n    body_face_color=\"#8189c9\",\n    body_edge_color=\"k\",\n    body_lw=0.25,\n    body_alpha=1.0,\n    body_zorder=0,\n    whisker_color=\"k\",\n    whisker_alpha=1.0,\n    whisker_lw=1,\n    whisker_zorder=1,\n    cap_color=\"k\",\n    cap_lw=0.25,\n    cap_zorder=1,\n    median_color=\"k\",\n    median_alpha=1.0,\n    median_lw=1.5,\n    median_bar_length=1.0,\n    median_zorder=10,\n    width=0.5,\n    scatter_face_color=\"k\",\n    scatter_edge_color=\"none\",\n    scatter_radius=5,\n    scatter_lw=0.25,\n    scatter_alpha=0.35,\n    scatter_width=0.5,\n    scatter=True,\n    scatter_zorder=3,\n    fill_box=True,\n    showcaps=False,\n    showfliers=False,\n    whis=(0, 100),\n    vert=True,\n):\n    \"\"\"\n    Example:\n        X = [1, 2]\n        Y = [np.random.normal(0.75, 0.12, size=50), np.random.normal(0.8, 0.20, size=25)]\n        fig, ax = plt.subplots(figsize=[1, 1])\n        for (x, y) in zip(X, Y):\n            place_boxplot(ax, x, y)\n    \"\"\"\n    parts = ax.boxplot(\n        y,\n        positions=[x],\n        widths=width,\n        showcaps=showcaps,\n        showfliers=showfliers,\n        whis=whis,\n        vert=vert,\n    )\n\n    # polish the body\n    b = parts[\"boxes\"][0]\n    b.set_color(body_edge_color)\n    b.set_alpha(body_alpha)\n    b.set_linewidth(body_lw)\n    b.set_zorder(body_zorder)\n    if fill_box:\n        if vert:\n            x0, x1 = b.get_xdata()[:2]\n            y0, y1 = b.get_ydata()[1:3]\n            r = Rectangle(\n                [x0, y0],\n                x1 - x0,\n                y1 - y0,\n                facecolor=body_face_color,\n                alpha=body_alpha,\n                edgecolor=\"none\",\n            )\n            ax.add_patch(r)\n        else:\n            x0, x1 = b.get_xdata()[1:3]\n            y0, y1 = b.get_ydata()[:2]\n            r = Rectangle(\n                [x0, y0],\n                x1 - x0,\n                y1 - y0,\n                facecolor=body_face_color,\n                alpha=body_alpha,\n                edgecolor=\"none\",\n            )\n            ax.add_patch(r)\n\n    # polish the whiskers\n    for w in parts[\"whiskers\"]:\n        w.set_color(whisker_color)\n        w.set_alpha(whisker_alpha)\n        w.set_linewidth(whisker_lw)\n        w.set_zorder(whisker_zorder)\n\n    # polish the caps\n    for c in parts[\"caps\"]:\n        c.set_color(cap_color)\n        c.set_linewidth(cap_lw)\n        c.set_zorder(cap_zorder)\n\n    # polish the median\n    m = parts[\"medians\"][0]\n    m.set_color(median_color)\n    m.set_linewidth(median_lw)\n    m.set_alpha(median_alpha)\n    m.set_zorder(median_zorder)\n    if median_bar_length is not None:\n        if vert:\n            x0, x1 = m.get_xdata()\n            m.set_xdata(\n                [\n                    x0 - 1 / 2 * (median_bar_length - 1) * (x1 - x0),\n                    x1 + 1 / 2 * (median_bar_length - 1) * (x1 - x0),\n                ]\n            )\n        else:\n            y0, y1 = m.get_ydata()\n            m.set_ydata(\n                [\n                    y0 - 1 / 2 * (median_bar_length - 1) * (y1 - y0),\n                    y1 + 1 / 2 * (median_bar_length - 1) * (y1 - y0),\n                ]\n            )\n\n    # scatter data\n    if scatter:\n        if vert:\n            x0, x1 = b.get_xdata()[:2]\n            ax.scatter(\n                np.random.uniform(\n                    x0 + 1 / 2 * (1 - scatter_width) * (x1 - x0),\n                    x1 - 1 / 2 * (1 - scatter_width) * (x1 - x0),\n                    size=len(y),\n                ),\n                y,\n                facecolor=scatter_face_color,\n                edgecolor=scatter_edge_color,\n                s=scatter_radius,\n                linewidth=scatter_lw,\n                zorder=scatter_zorder,\n                alpha=scatter_alpha,\n            )\n        else:\n            y0, y1 = b.get_ydata()[:2]\n            ax.scatter(\n                y,\n                np.random.uniform(\n                    y0 + 1 / 2 * (1 - scatter_width) * (y1 - y0),\n                    y1 - 1 / 2 * (1 - scatter_width) * (y1 - y0),\n                    size=len(y),\n                ),\n                facecolor=scatter_face_color,\n                edgecolor=scatter_edge_color,\n                s=scatter_radius,\n                linewidth=scatter_lw,\n                zorder=scatter_zorder,\n                alpha=scatter_alpha,\n            )\n</code></pre>"},{"location":"api/#labproject.plotting.place_violin","title":"<code>place_violin(ax, x, y, body_face_color='#8189c9', body_edge_color='k', body_lw=0.25, body_alpha=1.0, body_zorder=0, whisker_color='k', whisker_alpha=1.0, whisker_lw=1, whisker_zorder=1, cap_color='k', cap_lw=0.25, cap_zorder=1, median_color='k', median_alpha=1.0, median_lw=1.5, median_bar_length=1.0, median_zorder=10, width=0.5, scatter_face_color='k', scatter_edge_color='none', scatter_radius=5, scatter_lw=0.25, scatter_alpha=0.35, scatter_width=0.5, scatter=True, scatter_zorder=3, showextrema=True, showmedians=True, showmeans=False, vert=True)</code>","text":"Example <p>X = [1, 2] Y = [np.random.normal(0.75, 0.12, size=50), np.random.normal(0.8, 0.20, size=25)] fig, ax = plt.subplots(figsize=[1, 1]) for (x, y) in zip(X, Y):     place_violin(ax, x, y)</p> Source code in <code>labproject/plotting.py</code> <pre><code>def place_violin(\n    ax,\n    x,\n    y,\n    body_face_color=\"#8189c9\",\n    body_edge_color=\"k\",\n    body_lw=0.25,\n    body_alpha=1.0,\n    body_zorder=0,\n    whisker_color=\"k\",\n    whisker_alpha=1.0,\n    whisker_lw=1,\n    whisker_zorder=1,\n    cap_color=\"k\",\n    cap_lw=0.25,\n    cap_zorder=1,\n    median_color=\"k\",\n    median_alpha=1.0,\n    median_lw=1.5,\n    median_bar_length=1.0,\n    median_zorder=10,\n    width=0.5,\n    scatter_face_color=\"k\",\n    scatter_edge_color=\"none\",\n    scatter_radius=5,\n    scatter_lw=0.25,\n    scatter_alpha=0.35,\n    scatter_width=0.5,\n    scatter=True,\n    scatter_zorder=3,\n    showextrema=True,\n    showmedians=True,\n    showmeans=False,\n    vert=True,\n):\n    \"\"\"\n    Example:\n        X = [1, 2]\n        Y = [np.random.normal(0.75, 0.12, size=50), np.random.normal(0.8, 0.20, size=25)]\n        fig, ax = plt.subplots(figsize=[1, 1])\n        for (x, y) in zip(X, Y):\n            place_violin(ax, x, y)\n    \"\"\"\n    if not np.any(y):\n        return\n    parts = ax.violinplot(\n        y,\n        positions=[x],\n        widths=width,\n        showmedians=showmedians,\n        showmeans=showmeans,\n        showextrema=showextrema,\n        vert=vert,\n    )\n    # Color the bodies.\n    b = parts[\"bodies\"][0]\n    b.set_facecolor(body_face_color)\n    b.set_edgecolor(body_edge_color)\n    b.set_linewidth(body_lw)\n    b.set_alpha(body_alpha)\n    b.set_zorder(body_zorder)\n\n    # Color the lines.\n    if showextrema:\n        parts[\"cbars\"].set_color(whisker_color)\n        parts[\"cbars\"].set_alpha(whisker_alpha)\n        parts[\"cbars\"].set_linewidth(whisker_lw)\n        parts[\"cbars\"].set_zorder(whisker_zorder)\n        parts[\"cmaxes\"].set_color(cap_color)\n        parts[\"cmaxes\"].set_linewidth(cap_lw)\n        parts[\"cmaxes\"].set_zorder(cap_zorder)\n        parts[\"cmins\"].set_color(cap_color)\n        parts[\"cmins\"].set_linewidth(cap_lw)\n        parts[\"cmins\"].set_zorder(cap_zorder)\n\n    if showmeans:\n        parts[\"cmeans\"].set_color(median_color)\n        parts[\"cmeans\"].set_linewidth(median_lw)\n        parts[\"cmeans\"].set_alpha(median_alpha)\n        parts[\"cmeans\"].set_zorder(median_zorder)\n        if median_bar_length is not None:\n            if vert:\n                (_, y0), (_, y1) = parts[\"cmeans\"].get_segments()[0]\n                parts[\"cmeans\"].set_segments(\n                    [\n                        [\n                            [x - median_bar_length * width / 2, y0],\n                            [x + median_bar_length * width / 2, y1],\n                        ]\n                    ]\n                )\n            else:\n                (x0, _), (x1, _) = parts[\"cmeans\"].get_segments()[0]\n                parts[\"cmeans\"].set_segments(\n                    [\n                        [\n                            [x0, x - median_bar_length * width / 2],\n                            [x1, x + median_bar_length * width / 2],\n                        ]\n                    ]\n                )\n\n    if showmedians:\n        parts[\"cmedians\"].set_color(median_color)\n        parts[\"cmedians\"].set_alpha(median_alpha)\n        parts[\"cmedians\"].set_linewidth(median_lw)\n        parts[\"cmedians\"].set_zorder(median_zorder)\n        if median_bar_length is not None:\n            if vert:\n                (_, y0), (_, y1) = parts[\"cmedians\"].get_segments()[0]\n                parts[\"cmedians\"].set_segments(\n                    [\n                        [\n                            [x - median_bar_length * width / 2, y0],\n                            [x + median_bar_length * width / 2, y1],\n                        ]\n                    ]\n                )\n            else:\n                (x0, _), (x1, _) = parts[\"cmedians\"].get_segments()[0]\n                parts[\"cmedians\"].set_segments(\n                    [\n                        [\n                            [x0, x - median_bar_length * width / 2],\n                            [x1, x + median_bar_length * width / 2],\n                        ]\n                    ]\n                )\n\n    # scatter data\n    if scatter:\n        if vert:\n            ax.scatter(\n                np.random.uniform(\n                    x - width / 2 + 1 / 2 * (1 - scatter_width) * width,\n                    x + width / 2 - 1 / 2 * (1 - scatter_width) * width,\n                    size=len(y),\n                ),\n                y,\n                facecolor=scatter_face_color,\n                edgecolor=scatter_edge_color,\n                s=scatter_radius,\n                linewidth=scatter_lw,\n                alpha=scatter_alpha,\n                zorder=scatter_zorder,\n            )\n        else:\n            ax.scatter(\n                y,\n                np.random.uniform(\n                    x - width / 2 + 1 / 2 * (1 - scatter_width) * width,\n                    x + width / 2 - 1 / 2 * (1 - scatter_width) * width,\n                    size=len(y),\n                ),\n                facecolor=scatter_face_color,\n                edgecolor=scatter_edge_color,\n                s=scatter_radius,\n                linewidth=scatter_lw,\n                zorder=5,\n                alpha=scatter_alpha,\n            )\n</code></pre>"},{"location":"api/#labproject.plotting.plot_scaling_metric_dimensionality","title":"<code>plot_scaling_metric_dimensionality(dim_sizes, distances, errors, metric_name, dataset_name, ax=None, label=None, **kwargs)</code>","text":"<p>Plot the scaling of a metric with increasing dimensionality.</p> Source code in <code>labproject/plotting.py</code> <pre><code>def plot_scaling_metric_dimensionality(\n    dim_sizes,\n    distances,\n    errors,\n    metric_name,\n    dataset_name,\n    ax=None,\n    label=None,\n    **kwargs,\n):\n    \"\"\"Plot the scaling of a metric with increasing dimensionality.\"\"\"\n    if ax is None:\n        plt.plot(\n            dim_sizes,\n            distances,\n            label=metric_name if label is None else label,\n            **kwargs,\n        )\n        plt.fill_between(\n            dim_sizes,\n            distances - errors,\n            distances + errors,\n            alpha=0.2,\n            color=\"black\" if kwargs.get(\"color\") is None else kwargs.get(\"color\"),\n        )\n        plt.xlabel(\"Dimension\")\n        plt.ylabel(metric_name)\n        plt.title(f\"{metric_name} with increasing dimensionality size for {dataset_name}\")\n        plt.savefig(\n            os.path.join(\n                PLOT_PATH,\n                f\"{metric_name.lower().replace(' ', '_')}_dimensionality_size_{dataset_name.lower().replace(' ', '_')}.png\",\n            )\n        )\n        plt.close()\n    else:\n        ax.plot(\n            dim_sizes,\n            distances,\n            label=metric_name if label is None else label,\n            **kwargs,\n        )\n        ax.fill_between(\n            dim_sizes,\n            distances - errors,\n            distances + errors,\n            alpha=0.2,\n            color=\"black\" if kwargs.get(\"color\") is None else kwargs.get(\"color\"),\n        )\n        ax.set_xlabel(\"samples\")\n        ax.set_ylabel(\n            metric_name, color=\"black\" if kwargs.get(\"color\") is None else kwargs.get(\"color\")\n        )\n        return ax\n</code></pre>"},{"location":"api/#labproject.plotting.plot_scaling_metric_sample_size","title":"<code>plot_scaling_metric_sample_size(sample_size, distances, errors, metric_name, dataset_name, ax=None, label=None, **kwargs)</code>","text":"<p>Plot the behavior of a metric with number of samples.</p> Source code in <code>labproject/plotting.py</code> <pre><code>def plot_scaling_metric_sample_size(\n    sample_size,\n    distances,\n    errors,\n    metric_name,\n    dataset_name,\n    ax=None,\n    label=None,\n    **kwargs,\n):\n    \"\"\"Plot the behavior of a metric with number of samples.\"\"\"\n    if ax is None:\n        plt.plot(\n            sample_size,\n            distances,\n            label=metric_name if label is None else label,\n            **kwargs,\n        )\n        plt.fill_between(\n            sample_size,\n            distances - errors,\n            distances + errors,\n            alpha=0.2,\n            color=\"black\" if kwargs.get(\"color\") is None else kwargs.get(\"color\"),\n        )\n        plt.xlabel(\"samples\")\n        plt.ylabel(metric_name)\n        plt.title(f\"{metric_name} with increasing sample size for {dataset_name}\")\n        plt.savefig(\n            os.path.join(\n                PLOT_PATH,\n                f\"{metric_name.lower().replace(' ', '_')}_sample_size_{dataset_name.lower().replace(' ', '_')}.png\",\n            )\n        )\n        plt.close()\n    else:\n        ax.plot(\n            sample_size,\n            distances,\n            label=metric_name if label is None else label,\n            **kwargs,\n        )\n        ax.fill_between(\n            sample_size,\n            distances - errors,\n            distances + errors,\n            alpha=0.2,\n            color=\"black\" if kwargs.get(\"color\") is None else kwargs.get(\"color\"),\n        )\n        ax.set_xlabel(\"samples\")\n        ax.set_ylabel(\n            metric_name, color=\"black\" if kwargs.get(\"color\") is None else kwargs.get(\"color\")\n        )\n        return ax\n</code></pre>"},{"location":"api/#utils","title":"Utils","text":""},{"location":"api/#labproject.utils.get_cfg","title":"<code>get_cfg()</code>","text":"<p>This function returns the configuration file for the current experiment run.</p> <p>The configuration file is expected to be located at ../configs/conf_{name}.yaml, where name will match the name of the run_{name}.py file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the configuration file is not found</p> <p>Returns:</p> Name Type Description <code>OmegaConf</code> <code>OmegaConf</code> <p>Dictionary with the configuration parameters</p> Source code in <code>labproject/utils.py</code> <pre><code>def get_cfg() -&gt; OmegaConf:\n    \"\"\"This function returns the configuration file for the current experiment run.\n\n    The configuration file is expected to be located at ../configs/conf_{name}.yaml, where name will match the name of the run_{name}.py file.\n\n    Raises:\n        FileNotFoundError: If the configuration file is not found\n\n    Returns:\n        OmegaConf: Dictionary with the configuration parameters\n    \"\"\"\n    caller_frame = inspect.currentframe().f_back\n    filename = caller_frame.f_code.co_filename\n    name = filename.split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1]\n    try:\n        config = OmegaConf.load(CONF_PATH + f\"/conf_{name}.yaml\")\n        config.running_user = name\n    except FileNotFoundError:\n        msg = f\"Config file not found for {name}. Please create a config file at ../configs/conf_{name}.yaml\"\n        raise FileNotFoundError(msg)\n    return config\n</code></pre>"},{"location":"api/#labproject.utils.get_cfg_from_file","title":"<code>get_cfg_from_file(name)</code>","text":"<p>This function returns the configuration file for the current experiment run.</p> <p>The configuration file is expected to be located at ../configs/{name}.yaml .</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the configuration file is not found</p> <p>Returns:</p> Name Type Description <code>OmegaConf</code> <code>OmegaConf</code> <p>Dictionary with the configuration parameters</p> Source code in <code>labproject/utils.py</code> <pre><code>def get_cfg_from_file(name: str) -&gt; OmegaConf:\n    \"\"\"This function returns the configuration file for the current experiment run.\n\n    The configuration file is expected to be located at ../configs/{name}.yaml .\n\n    Raises:\n        FileNotFoundError: If the configuration file is not found\n\n    Returns:\n        OmegaConf: Dictionary with the configuration parameters\n    \"\"\"\n    try:\n        config = OmegaConf.load(CONF_PATH + f\"/{name}.yaml\")\n    except FileNotFoundError:\n        msg = f\"Config file not found for {name}. Please create a config file at ../configs/{name}.yaml\"\n        raise FileNotFoundError(msg)\n    return config\n</code></pre>"},{"location":"api/#labproject.utils.get_log_path","title":"<code>get_log_path(cfg, tag='', timestamp=True)</code>","text":"<p>Get the log path for the current experiment run. This log path is then used to save the numerical results of the experiment. Import this function in the run_{name}.py file and call it to get the log path.</p> Source code in <code>labproject/utils.py</code> <pre><code>def get_log_path(cfg, tag=\"\", timestamp=True):\n    \"\"\"\n    Get the log path for the current experiment run.\n    This log path is then used to save the numerical results of the experiment.\n    Import this function in the run_{name}.py file and call it to get the log path.\n    \"\"\"\n\n    # get datetime string\n    now = datetime.datetime.now()\n    if \"exp_log_name\" not in cfg:\n        exp_log_name = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    else:\n        exp_log_name = cfg.exp_log_name\n        # add datetime to the name\n        add_date = now.strftime(\"%Y-%m-%d_%H-%M-%S\") if timestamp else \"\"\n        exp_log_name = exp_log_name + tag + \"_\" + add_date\n    log_path = os.path.join(f\"results/{cfg.running_user}/{exp_log_name}.pkl\")\n    return log_path\n</code></pre>"},{"location":"api/#labproject.utils.load_experiments","title":"<code>load_experiments(cfg, tag='', now='')</code>","text":"<p>load the experiments to run</p> Source code in <code>labproject/utils.py</code> <pre><code>def load_experiments(cfg, tag=\"\", now=\"\"):\n    \"\"\"\n    load the experiments to run\n    \"\"\"\n    exp_log_name = cfg.exp_log_name\n    # add datetime to the name\n    exp_log_name = exp_log_name + tag + \"_\" + now\n    log_path = os.path.join(f\"results/{cfg.running_user}/{exp_log_name}\")\n    return log_path\n</code></pre>"},{"location":"api/#labproject.utils.set_seed","title":"<code>set_seed(seed)</code>","text":"<p>Set seed for reproducibility</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Integer seed</p> required Source code in <code>labproject/utils.py</code> <pre><code>def set_seed(seed: int) -&gt; None:\n    \"\"\"Set seed for reproducibility\n\n    Args:\n        seed (int): Integer seed\n    \"\"\"\n    torch.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    return seed\n</code></pre>"}]}